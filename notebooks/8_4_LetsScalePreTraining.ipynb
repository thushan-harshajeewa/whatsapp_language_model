{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First start by loading the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "tokenizer.load(model_file=\"../output/tokenizer/darija_tokenizer.model\")\n",
    "\n",
    "\n",
    "def get_vocab_size(tokenizer: RegexTokenizer) -> int:\n",
    "    vocab = tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create an instance of the `GPTLanguageModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(3647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.model import GPTLanguageModel\n",
    "\n",
    "block_size = 1024\n",
    "n_embd = 512\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.2\n",
    "batch_size = 4\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the encoded data as a memory mapped file object by using `mmap_mode='r'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "data = np.load('../output/encoded_data/encoded_atlaset.npy', mmap_mode='r')\n",
    "print('Data shape:', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the index for splitting the data, allocating 90% for training and the remaining 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(0.9*len(data))\n",
    "split_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_batch_for_loss_estimation` is used to get a random batch from any split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def get_batch_for_loss_estimation(split: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if split == 'train':\n",
    "        start_index = 0\n",
    "        end_index = split_index\n",
    "    else:\n",
    "        start_index = split_index\n",
    "        end_index = len(data)\n",
    "\n",
    "    available_blocks = (end_index - start_index - 1) // block_size\n",
    "    block_indices = torch.randint(0, available_blocks, (batch_size,))\n",
    "\n",
    "    x_batch, y_batch = [], []\n",
    "    for i in block_indices:\n",
    "        block_start = start_index + (i * block_size)\n",
    "        x_batch.append(data[block_start:block_start+block_size])\n",
    "        y_batch.append(data[block_start+1:block_start+block_size+1])\n",
    "\n",
    "    x_batch = np.array(x_batch)\n",
    "    y_batch = np.array(y_batch)\n",
    "\n",
    "    x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
    "    y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `estimate_loss` function is called periodically during training. Given the large dataset size, evaluation is performed on 1000 batches. Ideally, we would evaluate every batch in both the training and validation splits, but this isn't feasible without powerful hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss() -> Dict:\n",
    "    output = {}\n",
    "    eval_iters = 1000\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch_for_loss_estimation(split)\n",
    "            _, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        output[split] = losses.mean()\n",
    "    model.train()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `save_checkpoint` is used to save the model's weights and other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: GPTLanguageModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    file_path: str = \"checkpoint.pth\"\n",
    ") -> None:\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two approaches to training:  \n",
    "\n",
    "1. **Overlapping Batches:** With a block size of 1024, for example, the first input consists of the first 1024 tokens, the second input starts just one token later, and so on. This introduces overlap between training sequences but generates a large number of batches, making training slower.  \n",
    "\n",
    "2. **Non-Overlapping Batches:** Instead of shifting by one token, we advance by the full block size, avoiding overlap. This reduces the number of batches and speeds up training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overlapping batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# batch_size = 4\n",
    "# block_size = 1024\n",
    "# gradient_accumulation_steps = 8\n",
    "# eval_interval = 100\n",
    "# save_interval = 10000\n",
    "\n",
    "# # equivalent to len(data) - block_size\n",
    "# total_data_to_process = split_index - block_size\n",
    "# total_data_to_process_in_batches = total_data_to_process // batch_size\n",
    "\n",
    "# learning_rate = 3e-4\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# batches_processed = 0\n",
    "# train_losses, val_losses = [], []\n",
    "# optimizer.zero_grad(set_to_none=True)\n",
    "# for i in tqdm(\n",
    "#     iterable=range(0, total_data_to_process, batch_size),\n",
    "#     desc=\"Processing\",\n",
    "#     total=total_data_to_process_in_batches\n",
    "# ):\n",
    "#     # Load a batch of data\n",
    "#     x_batch, y_batch = [], []\n",
    "#     for j in range(i, i+batch_size):\n",
    "#         x_batch.append(data[j:j+block_size])\n",
    "#         y_batch.append(data[j+1:j+block_size+1])\n",
    "\n",
    "#     x_batch = np.array(x_batch)\n",
    "#     y_batch = np.array(y_batch)\n",
    "\n",
    "#     x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
    "#     y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
    "\n",
    "#     # Forward pass\n",
    "#     logits, loss = model(x_batch, y_batch)\n",
    "#     loss /= gradient_accumulation_steps\n",
    "#     loss.backward()\n",
    "\n",
    "#     # Gradient accumulation\n",
    "#     batches_processed += 1\n",
    "#     if batches_processed % gradient_accumulation_steps == 0:\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     if batches_processed % eval_interval == 0:\n",
    "#         losses = estimate_loss()\n",
    "#         print(\n",
    "#             f\"Batch {batches_processed}: \"\n",
    "#             f\"train loss {losses['train']:.4f}, \"\n",
    "#             f\"val loss {losses['val']:.4f}\"\n",
    "#         )\n",
    "#         train_losses.append(losses['train'])\n",
    "#         val_losses.append(losses['val'])\n",
    "\n",
    "#     # Save the model\n",
    "#     if batches_processed % save_interval == 0:\n",
    "#         save_checkpoint(\n",
    "#             model=model,\n",
    "#             optimizer=optimizer,\n",
    "#             epoch=batches_processed,\n",
    "#             loss=loss.item(),\n",
    "#             file_path=f\"../output/pre_training/run_1/checkpoint_{batches_processed}.pth\"\n",
    "#         )\n",
    "\n",
    "# if batches_processed % gradient_accumulation_steps != 0:\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Non-Overlapping batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "eval_interval = 3000\n",
    "save_interval = 10000\n",
    "\n",
    "# Calculate the number of complete non-overlapping blocks\n",
    "non_overlapping_blocks = (split_index - 1) // block_size\n",
    "total_batches = non_overlapping_blocks // batch_size\n",
    "\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "batches_processed = 0\n",
    "train_losses, val_losses = [], []\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "for i in tqdm(\n",
    "    iterable=range(0, non_overlapping_blocks, batch_size),\n",
    "    desc=\"Processing\",\n",
    "    total=total_batches\n",
    "):\n",
    "    # Load a batch of non-overlapping blocks\n",
    "    x_batch, y_batch = [], []\n",
    "    for j in range(batch_size):\n",
    "        if i+j < non_overlapping_blocks:\n",
    "            block_start = (i+j) * block_size\n",
    "            x = data[block_start:block_start+block_size]\n",
    "            y = data[block_start+1:block_start+block_size+1]\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "\n",
    "    if len(x_batch) == 0:\n",
    "        continue\n",
    "\n",
    "    x_batch = np.array(x_batch)\n",
    "    y_batch = np.array(y_batch)\n",
    "\n",
    "    x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
    "    y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    loss /= gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient accumulation\n",
    "    batches_processed += 1\n",
    "    if batches_processed % gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Evaluate the model\n",
    "    if batches_processed % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"Batch {batches_processed}: \"\n",
    "            f\"train loss {losses['train']:.4f}, \"\n",
    "            f\"val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "\n",
    "    # Save the model\n",
    "    if batches_processed % save_interval == 0:\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            epoch=batches_processed,\n",
    "            loss=loss.item(),\n",
    "            file_path=f\"../output/pre_training/run_1/checkpoint_{batches_processed}.pth\"\n",
    "        )\n",
    "\n",
    "if batches_processed % gradient_accumulation_steps != 0:\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "save_checkpoint(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epoch=batches_processed,\n",
    "    loss=loss.item(),\n",
    "    file_path=f\"../output/pre_training/run_1/checkpoint_{batches_processed}.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylim(0)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try generating text using the base model to assess its ability to model the language. Keep in mind that it hasn't been fine-tuned for conversation, so it won't behave like a helpful assistant just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer.encode(\"ماهي الأسباب الرئيسية اللي خلاو الفرنسيين\")\n",
    "input_tokens = torch.tensor(\n",
    "    input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tokens=input_tokens, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
