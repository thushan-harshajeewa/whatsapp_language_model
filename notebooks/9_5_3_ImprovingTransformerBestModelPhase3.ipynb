{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First start by loading the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "tokenizer.load(model_file=\"../output/tokenizer/darija_tokenizer.model\")\n",
    "\n",
    "\n",
    "def get_vocab_size(tokenizer: RegexTokenizer) -> int:\n",
    "    vocab = tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create an instance of the `GPTLanguageModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(3647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.best_model_phase_3 import GPTLanguageModel\n",
    "\n",
    "block_size = 1024\n",
    "n_embd = 256\n",
    "n_head = 16\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "batch_size = 4\n",
    "q_comp_dim = n_embd // 2\n",
    "head_dim = n_embd // n_head\n",
    "kv_comp_dim = 4 * head_dim\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    block_size=block_size,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    "    device=device,\n",
    "    q_compression_dim=q_comp_dim,\n",
    "    kv_compression_dim=kv_comp_dim,\n",
    ").to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the encoded data as a memory mapped file object by using `mmap_mode='r'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "data = np.load(\"../output/encoded_data/encoded_atlaset.npy\", mmap_mode=\"r\")\n",
    "print(\"Data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the index for splitting the data, allocating 90% for training and the remaining 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(0.9 * len(data))\n",
    "split_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `estimate_loss` function is called periodically during training. Given the large dataset size, evaluation is performed on 1000 batches. Ideally, we would evaluate every batch in both the training and validation splits, but this isn't feasible without powerful hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "\n",
    "eval_batches = 1000\n",
    "\n",
    "\n",
    "def get_evaluation_indices(split: str, eval_batches: int) -> torch.Tensor:\n",
    "    if split == \"train\":\n",
    "        start, end = 0, split_index\n",
    "    else:\n",
    "        start, end = split_index, len(data)\n",
    "    num_blocks = (end - start - 1) // block_size\n",
    "    return torch.randint(0, num_blocks, (eval_batches,))\n",
    "\n",
    "\n",
    "eval_indices = {\n",
    "    \"train\": get_evaluation_indices(\"train\", eval_batches),\n",
    "    \"val\": get_evaluation_indices(\"val\", eval_batches),\n",
    "}\n",
    "\n",
    "\n",
    "def get_batch_for_loss_estimation(\n",
    "    split: str, block_indices: list[int]\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    start = 0 if split == \"train\" else split_index\n",
    "    x_batch, y_batch = [], []\n",
    "    for i in block_indices:\n",
    "        block_start = start + (i * block_size)\n",
    "        x = data[block_start : block_start + block_size]\n",
    "        y = data[block_start + 1 : block_start + block_size + 1]\n",
    "        x_batch.append(x)\n",
    "        y_batch.append(y)\n",
    "\n",
    "    x_batch = torch.tensor(np.array(x_batch), dtype=torch.long).to(device)\n",
    "    y_batch = torch.tensor(np.array(y_batch), dtype=torch.long).to(device)\n",
    "    return x_batch, y_batch\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss() -> Dict:\n",
    "    model.eval()\n",
    "    output = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        indices = eval_indices[split]\n",
    "        for idx in indices:\n",
    "            x, y = get_batch_for_loss_estimation(split, [idx.item()])\n",
    "            _, loss = model(x, y)\n",
    "            losses.append(loss.item())\n",
    "        output[split] = sum(losses) / len(losses)\n",
    "    model.train()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `save_checkpoint` is used to save the model's weights and other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: GPTLanguageModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    file_path: str = \"checkpoint.pth\",\n",
    ") -> None:\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two approaches to training:  \n",
    "\n",
    "1. **Overlapping Batches:** With a block size of 1024, for example, the first input consists of the first 1024 tokens, the second input starts just one token later, and so on. This introduces overlap between training sequences but generates a large number of batches, making training slower.  \n",
    "\n",
    "2. **Non-Overlapping Batches:** Instead of shifting by one token, we advance by the full block size, avoiding overlap. This reduces the number of batches and speeds up training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overlapping batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# batch_size = 4\n",
    "# block_size = 1024\n",
    "# gradient_accumulation_steps = 8\n",
    "# eval_interval = 100\n",
    "# save_interval = 10000\n",
    "\n",
    "# # equivalent to len(data) - block_size\n",
    "# total_data_to_process = split_index - block_size\n",
    "# total_data_to_process_in_batches = total_data_to_process // batch_size\n",
    "\n",
    "# learning_rate = 3e-4\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# batches_processed = 0\n",
    "# train_losses, val_losses = [], []\n",
    "# optimizer.zero_grad(set_to_none=True)\n",
    "# for i in tqdm(\n",
    "#     iterable=range(0, total_data_to_process, batch_size),\n",
    "#     desc=\"Processing\",\n",
    "#     total=total_data_to_process_in_batches\n",
    "# ):\n",
    "#     # Load a batch of data\n",
    "#     x_batch, y_batch = [], []\n",
    "#     for j in range(i, i+batch_size):\n",
    "#         x_batch.append(data[j:j+block_size])\n",
    "#         y_batch.append(data[j+1:j+block_size+1])\n",
    "\n",
    "#     x_batch = np.array(x_batch)\n",
    "#     y_batch = np.array(y_batch)\n",
    "\n",
    "#     x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
    "#     y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
    "\n",
    "#     # Forward pass\n",
    "#     logits, loss = model(x_batch, y_batch)\n",
    "#     loss /= gradient_accumulation_steps\n",
    "#     loss.backward()\n",
    "\n",
    "#     # Gradient accumulation\n",
    "#     batches_processed += 1\n",
    "#     if batches_processed % gradient_accumulation_steps == 0:\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     if batches_processed % eval_interval == 0:\n",
    "#         losses = estimate_loss()\n",
    "#         print(\n",
    "#             f\"Batch {batches_processed}: \"\n",
    "#             f\"train loss {losses['train']:.4f}, \"\n",
    "#             f\"val loss {losses['val']:.4f}\"\n",
    "#         )\n",
    "#         train_losses.append(losses['train'])\n",
    "#         val_losses.append(losses['val'])\n",
    "\n",
    "#     # Save the model\n",
    "#     if batches_processed % save_interval == 0:\n",
    "#         save_checkpoint(\n",
    "#             model=model,\n",
    "#             optimizer=optimizer,\n",
    "#             epoch=batches_processed,\n",
    "#             loss=loss.item(),\n",
    "#             file_path=f\"../output/pre_training/run_1/checkpoint_{batches_processed}.pth\"\n",
    "#         )\n",
    "\n",
    "# if batches_processed % gradient_accumulation_steps != 0:\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Non-Overlapping batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Set the precision for float32 matrix multiplication\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Training Hyperparameters\n",
    "num_epochs = 1\n",
    "gradient_accumulation_steps = 8\n",
    "eval_interval = 5000 // gradient_accumulation_steps\n",
    "save_interval = 10000 // gradient_accumulation_steps\n",
    "max_learning_rate = 3e-4\n",
    "min_learning_rate = 3e-5\n",
    "warmup_ratio = 0.1\n",
    "\n",
    "# Calculate the number of complete non-overlapping blocks\n",
    "non_overlapping_blocks = (split_index - 1) // block_size\n",
    "total_batches_per_epoch = non_overlapping_blocks // batch_size\n",
    "\n",
    "# Scheduler calculation\n",
    "max_iters = total_batches_per_epoch * num_epochs\n",
    "optimizer_steps_total = max_iters // gradient_accumulation_steps\n",
    "warmup_iters = int(warmup_ratio * optimizer_steps_total)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_learning_rate)\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer=optimizer,\n",
    "    T_max=optimizer_steps_total - warmup_iters,\n",
    "    eta_min=min_learning_rate,\n",
    ")\n",
    "\n",
    "# Training State\n",
    "batches_processed_total = 0\n",
    "optimizer_steps = 0\n",
    "train_losses, val_losses = [], []\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "print(f\"Total non-overlapping blocks: {non_overlapping_blocks}\")\n",
    "print(f\"Batches per epoch: {total_batches_per_epoch}\")\n",
    "print(f\"Total optimizer steps (max_iters): {max_iters}\")\n",
    "print(f\"Warmup iterations: {warmup_iters}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(\n",
    "        iterable=range(0, non_overlapping_blocks, batch_size),\n",
    "        desc=f\"Epoch {epoch + 1}/{num_epochs}\",\n",
    "        total=total_batches_per_epoch,\n",
    "    )\n",
    "\n",
    "    for i in pbar:\n",
    "        x_batch, y_batch = [], []\n",
    "        batch_end_index = min(i + batch_size, non_overlapping_blocks)\n",
    "\n",
    "        for block_idx in range(i, batch_end_index):\n",
    "            block_start = block_idx * block_size\n",
    "            x = data[block_start : block_start + block_size]\n",
    "            y = data[block_start + 1 : block_start + block_size + 1]\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "\n",
    "        if not x_batch:\n",
    "            continue\n",
    "\n",
    "        x_batch = np.array(x_batch)\n",
    "        y_batch = np.array(y_batch)\n",
    "\n",
    "        x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
    "        y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, loss = model(x_batch, y_batch)\n",
    "        loss_val = loss.item()\n",
    "        loss /= gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        batches_processed_total += 1\n",
    "\n",
    "        if batches_processed_total % gradient_accumulation_steps == 0:\n",
    "            optimizer_steps += 1\n",
    "\n",
    "            if optimizer_steps < warmup_iters:\n",
    "                learning_rate = max_learning_rate * optimizer_steps / warmup_iters\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group[\"lr\"] = learning_rate\n",
    "            elif optimizer_steps == warmup_iters:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group[\"lr\"] = max_learning_rate\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            if optimizer_steps >= warmup_iters:\n",
    "                scheduler.step()\n",
    "\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            pbar.set_postfix(\n",
    "                loss=f\"{loss_val:.4f}\",\n",
    "                lr=f\"{current_lr:.6f}\",\n",
    "                step=f\"{optimizer_steps}/{optimizer_steps_total}\",\n",
    "            )\n",
    "\n",
    "            if optimizer_steps % eval_interval == 0:\n",
    "                eval_losses = estimate_loss()\n",
    "                print(\n",
    "                    f\"\\nStep {optimizer_steps}: \"\n",
    "                    f\"train loss {eval_losses['train']:.4f}, \"\n",
    "                    f\"val loss {eval_losses['val']:.4f}, \"\n",
    "                    f\"lr {current_lr:.6f}\"\n",
    "                )\n",
    "                train_losses.append(eval_losses[\"train\"])\n",
    "                val_losses.append(eval_losses[\"val\"])\n",
    "                model.train()\n",
    "\n",
    "            if optimizer_steps % save_interval == 0:\n",
    "                print(f\"\\nSaving checkpoint at step {optimizer_steps}...\")\n",
    "                # save_checkpoint(\n",
    "                #     model=model,\n",
    "                #     optimizer=optimizer,\n",
    "                #     epoch=epoch + 1,\n",
    "                #     loss=loss_val,\n",
    "                #     file_path=f\"../output/pre_training/multi_head_latent_attention/checkpoint_step_{optimizer_steps}.pth\"\n",
    "                # )\n",
    "\n",
    "if batches_processed_total % gradient_accumulation_steps != 0:\n",
    "    optimizer_steps += 1\n",
    "    if optimizer_steps < warmup_iters:\n",
    "        learning_rate = max_learning_rate * optimizer_steps / warmup_iters\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = learning_rate\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    print(f\"\\nPerformed final optimizer step ({optimizer_steps}).\")\n",
    "    if optimizer_steps >= warmup_iters:\n",
    "        scheduler.step()\n",
    "\n",
    "print(\"\\nSaving final checkpoint...\")\n",
    "final_loss = estimate_loss()[\"val\"]\n",
    "# save_checkpoint(\n",
    "#     model=model,\n",
    "#     optimizer=optimizer,\n",
    "#     epoch=num_epochs,\n",
    "#     loss=final_loss,\n",
    "#     file_path=f\"../output/pre_training/multi_head_latent_attention/checkpoint_final_step_{optimizer_steps}.pth\"\n",
    "# )\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylim(0)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../loss_values/final_model_phase_3/train_losses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_losses, f)\n",
    "with open(\"../loss_values/final_model_phase_3/val_losses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_losses, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
