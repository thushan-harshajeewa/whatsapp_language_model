{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec87cfe0",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c537e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55a199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "tokenizer.load(model_file=\"../output/tokenizer/darija_tokenizer.model\")\n",
    "\n",
    "\n",
    "def get_vocab_size(tokenizer: RegexTokenizer) -> int:\n",
    "    vocab = tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48a79bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.232266 M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformer.best_model_phase_4 import GPTLanguageModel\n",
    "\n",
    "block_size = 1024\n",
    "n_embd = 256\n",
    "n_head = 16\n",
    "num_kv_heads = 4\n",
    "n_layer = 4\n",
    "batch_size = 4\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "q_compression_dim = n_embd // 2\n",
    "head_dim = n_embd // n_head\n",
    "kv_compression_dim = 4 * head_dim\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    device=device,\n",
    "    q_compression_dim=q_compression_dim,\n",
    "    kv_compression_dim=kv_compression_dim,\n",
    ").to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f455b6",
   "metadata": {},
   "source": [
    "## Calculate tokens/second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65f26c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 1 warm-up run(s)...\n",
      "\n",
      "Starting 3 timed runs to measure throughput...\n",
      "\n",
      "Run 1/3 - Time taken: 2.14s, Throughput: 466.76 tokens/sec\n",
      "Run 2/3 - Time taken: 2.14s, Throughput: 467.36 tokens/sec\n",
      "Run 3/3 - Time taken: 2.16s, Throughput: 462.06 tokens/sec\n",
      "\n",
      "Average throughput: 465.39 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_runs = 3\n",
    "warmup_runs = 1\n",
    "num_generated_tokens = 1000\n",
    "\n",
    "query = \"salam labas\"\n",
    "tokens_input = tokenizer.encode(query, allowed_special=\"all\")\n",
    "tokens_input = torch.tensor(tokens_input, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "throughputs = []\n",
    "\n",
    "print(f\"Performing {warmup_runs} warm-up run(s)...\")\n",
    "for _ in range(warmup_runs):\n",
    "    _ = model.advanced_generation(\n",
    "        input_tokens=tokens_input.clone(), max_new_tokens=num_generated_tokens\n",
    "    )\n",
    "\n",
    "print(f\"\\nStarting {num_runs} timed runs to measure throughput...\\n\")\n",
    "for i in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    generated_sequence = model.advanced_generation(\n",
    "        input_tokens=tokens_input, max_new_tokens=num_generated_tokens\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    duration = end_time - start_time\n",
    "    if duration > 0:\n",
    "        run_throughput = num_generated_tokens / duration\n",
    "        throughputs.append(run_throughput)\n",
    "        print(\n",
    "            f\"Run {i + 1}/{num_runs} - Time taken: {duration:.2f}s, Throughput: {run_throughput:.2f} tokens/sec\"\n",
    "        )\n",
    "\n",
    "if throughputs:\n",
    "    avg_throughput = sum(throughputs) / len(throughputs)\n",
    "    print(f\"\\nAverage throughput: {avg_throughput:.2f} tokens/sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
